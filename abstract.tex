
%\boldmath
%In general, most of compiler optimizations focus on improving the execution time and performance of compiled code. However, this may be so expensive at the expense of resource usage and may induce to compiler bugs or crashes. With the increasing of resource usage, it is important to evaluate the compiler behavior. Indeed, in resource-constrained environment, compiler optimizations may lead to memory leaks or execution bottlenecks. So, a fine-grained understanding of resource consumption and analysis of compilerâ€™s behavior regarding optimizations becomes necessary. As well, compilers may have a huge number of optimization combinations and it very hard and time-consuming to find the optimal sequence of optimization options that satisfies user key objective. In this paper, we propose DECO (Diversity-based Exploration of Compiler Optimizations), an automatic generator of compiler optimization sequences based on sequences diversity. In this approach, we apply the Novelty Search (NS) technique to determine the optimal sequence that could be used to produce a code which consumes less system resources. To do so, we explore the search space of possible optimization combinations by considering diversity of sequences as the unique objective function to be optimized. In fact, instead of having a fitness-based selection, we select optimization sequences based on a novelty score showing how different they are compared to all other solutions evaluated so far. We conduct experiments by providing a quantitative study of the impact of compiler optimizations explored by NS on non-functional properties like CPU and memory consumption. We run experiments on a commonly used set of benchmarks and we identified the optimal set of optimizations regarding performance. The results show that...
%In search-based structural testing, metaheuristic search techniques have been frequently used to automate the test data generation. In Genetic Algorithms (GAs) for example, test data are rewarded on the basis of an objective function that represents generally the number of statements or branches covered. However, owing to the wide diversity of possible test data values, it is hard to find the set of test data that can satisfy a specific coverage criterion. In this paper, we introduce the use of Novelty Search (NS) algorithm to the test data generation problem based on statement-covered criteria. We believe that such approach to test data generation is attractive because it allows the exploration of the huge space of test data within the input domain. In this approach, we seek to explore the search space without regard to any objectives. In fact, instead of having a fitness-based selection we select test cases based on a novelty score showing how different they are compared to all other solutions evaluated so far.

\begin{abstract}


Compiler users tend to improve software programs in a safe and profitable way.
Generally, they can apply different optimization techniques to generate efficient code with respect to non-functional properties such as memory consumption, execution time, code size, among others. However, due to the huge number of optimizations provided by modern compilers, finding the best optimization sequence for a specific objective and a given program is more and more challenging. 

This paper proposes NOTICE, a component-based framework for non-functional testing of compilers through the monitoring of generated code in a controlled sand-boxing environment.
NOTICE is an on-demand tool that employs different mono and multi-objective evolutionary search algorithms to construct optimization sequences that satisfy user key objectives (execution time, CPU or memory usage, etc.).
In our approach, we focus on the relationship between runtime execution of optimized code and resource consumption profiles (CPU and memory usage) by providing a fine-grained understanding and analysis of compilers behavior regarding optimizations.
We evaluate the effectiveness of our approach by verifying the optimizations performed by GCC compiler.
Our experimental results show that our approach is able to auto-tune compilers and construct optimizations that yield to better performance results than standard optimization levels.
We also demonstrate that NOTICE can be used to automatically construct optimization levels that represent optimal trade-offs between multiple non-functional properties, such as execution time, memory usage, CPU consumption, etc.



 



%they are highly dependent on target platforms
\end{abstract}
\smallskip
\noindent \textbf{Keywords.} \textit{software quality, non-functional properties, compilers, testing}.
% HD-Services will run within a heterogeneous and distributed infrastructure. To facilitate the testing of this kind of services, we need to deploy HD-services on an elastic infrastructure that provides preconfigured virtual server images, storage and network connectivity that may be provisioned by HD-Developers. Monitoring information should also be provided to inform about resource utilization required/needed and to automate the resource management. For this purpose, we propose a testing infrastructure based on docker environment. Docker will automate the deployment of applications inside software containers. It will simplify the creation of highly distributed systems by allowing multiple applications to run autonomously on a server (basically a cloud server). Docker will provide a platform as a service (PaaS) style of deployment for HD-Services. Consequently, we will rely on this technology and benefit from all its advantages to:
%1- Deploy preconfigured application to test within docker containers
%2- Automate test suites generation
%3- Monitor service containers
%4- Limit and manage resources for each running container
%5- Gather performance metrics (CPU, Memory, I/O, etc.)
%As a consequence, we are going to integrate a collection of docker technologies to define the adequate infrastructure for testing HD-Services.



%In model-driven engineering, developers use different code generators to translate source programs represented in a graphical modeling language into general purpose programming languages such as C, Java, C++,etc.  These code generators serve as a basis to target different ranges of platforms. Many technologies, such as Docker containers, provide new opportunities to automate the deployment of produced code into a distributed and heterogeneous component-based infrastructure. In fact, during the code generation process, different optimizations may be applied for code transformation. For example, embedded systems for which code is generated often have limited resources. Therefore, optimization techniques must be applied whenever possible to generate efficient code with respect to memory consumption, execution time, disk writing speed, among others. Sometimes, optimizations can even lead to memory leaks or execution bottlenecks especially for resource-constrained systems. So, to ensure the efficiency of generated code, deployed components must be checked and verified regarding their non-functional behavior. This paper describes a component-based tool for testing and monitoring generated code. It provides a fine-grained understanding of resource consumption and analysis of components behavior regarding optimizations. We evaluate the effectiveness of our test approach by means of testing optimizations performed by the GCC compiler, a widely used compiler in software engineering community. We present as well a number of case studies, in which the tool was successfully used.